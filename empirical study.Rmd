---
title: "Empirical study"
output: html_document
date: "2024-07-24"
---
empirical study
```{r}
Sys.setenv(LANG = "EN")
library(dplyr)
library(kableExtra)
library(MASS)
library(glmnet)
library(caret)
library(grf)
library(DoubleML)
library(mlr3)
library(mlr3learners)
# generate sample data
set.seed(123)
n <- 1200
c <- 1.9
Z <- rexp(n, 0.5)
X_1 <- rexp(n, 3)
X_2 <- rnorm(n, 10, 2)
X_3 <- rnorm(n, 5, 0.5)
theta <- 5
T <- ifelse(Z >= c, 1, 0)
Y <- theta * T + 2 * Z + 1.5 * X_1 - 2 * X_2 + 3 * X_3 + 
     1.2 * Z^2 - 0.5 * X_1^2 + 0.8 * X_2 * X_3 +
     0.3 * T * Z + 0.7 * sin(Z) + 0.4 * log(abs(X_1) + 1) + rnorm(n)
data <- data.frame(Y, T, Z, X_1, X_2, X_3)
```

```{r}
#Global Linear Regression 
l_ml <- lm(Y ~ T + Z + X_1 + X_2 + X_3, data = data)
l_int_ml <- lm(Y ~ T + Z + I(Z * T) + X_1 + X_2 + X_3, data = data)
qua_ml <- lm(Y ~ T + Z + I(Z^2) + X_1 + X_2 + X_3, data = data)
qua_int_ml <- lm(Y ~ T + Z + I(Z^2) + I(Z * T) + I(Z^2 * T) +  X_1 + X_2 + X_3, data = data)
cub_ml <- lm(Y ~ T + Z + I(Z^2) + I(Z^3) + X_1 + X_2 + X_3, data = data)
cub_int_model <- lm(Y ~ T + Z + I(Z^2) + I(Z^3) + I(Z * T) + I(Z^2 * T) + I(Z^3 * T) +  X_1 + X_2 + X_3, data = data)
l_theta_c <- coef(l_ml)["T"]
l_int_theta_c <- coef(l_int_ml)["T"]
qua_theta_c <- coef(qua_ml)["T"]
qua_int_theta_c <- coef(qua_int_ml)["T"]
cub_theta_c <- coef(cub_ml)["T"]
cub_int_theta_c <- coef(cub_int_model)["T"]
k <- 10
data_binned <- data %>%
  mutate(bin = sample(1:k, n(), replace = TRUE))
data_binned$bin <- factor(data_binned$bin)
indicator_vars <- model.matrix(~ bin, data = data_binned)[, -c(1, 2)] 
data_binned <- cbind(data_binned, indicator_vars)

model1 <- lm(Y ~ T + Z +  X_1 + X_2 + X_3, data = data_binned)
model2 <- lm(Y ~ T + Z + I(Z * T) +  X_1 + X_2 + X_3, data = data_binned)
model3 <- lm(Y ~ T + Z + I(Z^2) +  X_1 + X_2 + X_3, data = data_binned)
model4 <- lm(Y ~ T + Z + I(Z^2) + I(Z * T) + I(Z^2 * T) +  X_1 + X_2 + X_3, data = data_binned)
model5 <- lm(Y ~ T + Z + I(Z^2) + I(Z^3) +  X_1 + X_2 + X_3, data = data_binned)
model6 <- lm(Y ~ T + Z + I(Z^2) + I(Z^3) + I(Z * T) + I(Z^2 * T) + I(Z^3 * T) +  X_1 + X_2 + X_3, data = data_binned)
T_bin <- ifelse(data_binned[,3] >= c, 1, 0)
X_bin_1 <- data_binned[, 4]
X_bin_2 <- data_binned[, 5]
X_bin_3 <- data_binned[, 6]
Z_bin <- data_binned[, 3]
Y_bin <- data_binned[, 1]
model1_i <- lm(Y_bin ~ T_bin + Z_bin + X_bin_1 + X_bin_2 + X_bin_3 + bin3 + bin4 + bin5 + bin6 + bin7 + bin8 + bin9 + bin10, data = data_binned)
model2_i <- lm(Y_bin ~ T_bin + Z_bin + I(Z_bin * T_bin) + X_bin_1 + X_bin_2 + X_bin_3 + bin3 + bin4 + bin5 + bin6 + bin7 + bin8 + bin9 + bin10, data = data_binned)
model3_i <- lm(Y_bin ~ T_bin + Z_bin + I(Z_bin^2) +  X_bin_1 + X_bin_2 + X_bin_3 + bin3 + bin4 + bin5 + bin6 + bin7 + bin8 + bin9 + bin10, data = data_binned)
model4_i <- lm(Y_bin ~ T_bin + Z_bin + I(Z_bin^2) + I(Z_bin * T_bin) + I(Z_bin^2 * T_bin) +  X_bin_1 + X_bin_2 + X_bin_3 + bin3 + bin4 + bin5 + bin6 + bin7 + bin8 + bin9 + bin10, data = data_binned)
model5_i <- lm(Y_bin ~ T_bin + Z_bin + I(Z_bin^2) + I(Z_bin^3) +  X_bin_1 + X_bin_2 + X_bin_3 + bin3 + bin4 + bin5 + bin6 + bin7 + bin8 + bin9 + bin10, data =   data_binned)
model6_i <- lm(Y_bin ~ T_bin + Z_bin + I(Z_bin^2) + I(Z_bin^3) + I(Z_bin * T_bin) + I(Z_bin^2 * T_bin) + I(Z_bin^3 * T_bin) +  X_bin_1 + X_bin_2 + X_bin_3 + bin3 + bin4 + bin5 + bin6 + bin7 + bin8 + bin9 + bin10, data = data_binned)
R2_r_1 <- summary(model1)$r.squared
R2_u_1 <- summary(model1_i)$r.squared
R2_r_2 <- summary(model2)$r.squared
R2_u_2 <- summary(model2_i)$r.squared
R2_r_3 <- summary(model3)$r.squared
R2_u_3 <- summary(model3_i)$r.squared
R2_r_4 <- summary(model4)$r.squared
R2_u_4 <- summary(model4_i)$r.squared
R2_r_5 <- summary(model5)$r.squared
R2_u_5 <- summary(model5_i)$r.squared
R2_r_6 <- summary(model6)$r.squared
R2_u_6 <- summary(model6_i)$r.squared
F_1 <- ((R2_u_1 - R2_r_1) / (k - 2)) / ((1 - R2_u_1) / (n - k - 1))
F_2 <- ((R2_u_2 - R2_r_2) / (k - 2)) / ((1 - R2_u_2) / (n - k - 1))
F_3 <- ((R2_u_3 - R2_r_3) / (k - 2)) / ((1 - R2_u_3) / (n - k - 1))
F_4 <- ((R2_u_4 - R2_r_4) / (k - 2)) / ((1 - R2_u_4) / (n - k - 1))
F_5 <- ((R2_u_5 - R2_r_5) / (k - 2)) / ((1 - R2_u_5) / (n - k - 1))
F_6 <- ((R2_u_6 - R2_r_6) / (k - 2)) / ((1 - R2_u_6) / (n - k - 1))
p_1 <- pf(F_1, df1 = k - 2, df2 = n - k - 1, lower.tail = FALSE)
p_2 <- pf(F_2, df1 = k - 2, df2 = n - k - 1, lower.tail = FALSE)
p_3 <- pf(F_3, df1 = k - 2, df2 = n - k - 1, lower.tail = FALSE)
p_4 <- pf(F_4, df1 = k - 2, df2 = n - k - 1, lower.tail = FALSE)
p_5 <- pf(F_5, df1 = k - 2, df2 = n - k - 1, lower.tail = FALSE)
p_6 <- pf(F_6, df1 = k - 2, df2 = n - k - 1, lower.tail = FALSE)
results_gl <- data.frame(
  Model = c("Linear", "Linear Interaction", "Quadratic", "Quadratic Interaction", "Cubic", "Cubic Interaction"),
  TE = c(l_theta_c, l_int_theta_c, qua_theta_c, qua_int_theta_c, cub_theta_c, cub_int_theta_c), 
  p.value = c(p_1, p_2, p_3, p_4, p_5, p_6)
)
results_gl
```

```{r}
#Local Linear Regression 
h_n <- c(0.1, 0.5, 1, 1.7, 2.5)

cross_validation <- function(h_i, data) {
  data_left <- data[data$Z < c, ]
  Y_data_left <- data_left[, 1]
  n <- nrow(data_left)
  prediction <- rep(NA, n)
  for (i in 3:n) {
    subset_left <- data_left[0, ]
    for (j in 1:nrow(data_left)) {
      value <- data_left$Z[j]
      bound <- data_left$Z[i]
      if (value >= (bound - h_i) & value < bound) {
        subset_left <- rbind(subset_left, data_left[j, ])
      }
    }
    if(nrow(subset_left) >= 2){
      fit_left <- lm(Y ~ Z +  X_1 + X_2 + X_3, data = subset_left)
      coefficients <- coef(fit_left)
      newdata <- data_left[i, ]
      X <- c(intercept = 1, newdata$Z, newdata$X_1, newdata$X_2, newdata$X_3)
      pred_left <- sum(coefficients * X)
      prediction[i] <- pred_left
    }
  }
  na_indices <- which(is.na(prediction))
  prediction_clean <- prediction[!is.na(prediction)]
  Y_data_left <- Y_data_left[-na_indices]
  mse <- mean((Y_data_left - prediction_clean)^2)
  return(mse)
}

result_mse <- sapply(h_n, cross_validation, data = data)
optimal_h_n <- h_n[which.min(result_mse)]

triangular_kernel <-function(x){
  return (pmax(1 - abs(x), 0))
}
local_linear_regression <- function(h, data){
  weights <- triangular_kernel(abs(Z - c) / h)
  model <- lm(Y ~ T + pmax((Z - c), 0) + pmin((Z - c), 0) +  X_1 + X_2 + X_3, data = data, weights = weights)
  theta_c <- summary(model)$coefficients["T", "Estimate"]
  return(theta_c)
}
result_llr <- sapply(h_n, local_linear_regression, data = data)

results_llr <- data.frame(
  Bandwidth = h_n,
  N = rep(length(data$Y), length(h_n)), 
  MSE = result_mse, 
  TE = result_llr
)

results_llr
```

```{r}
#DML Partial Linear Model
set.seed(999)
# Lasso learners for the nuisance functions
ml_l_lasso <- lrn("regr.cv_glmnet", alpha = 1)
ml_m_lasso <- lrn("classif.cv_glmnet", alpha = 1, predict_type = "prob")

#Initialize a DoubleMLPLR object
dml_data <- DoubleMLData$new(data, y_col = "Y", d_cols = "T", x_cols = c("Z", "X_1", "X_2", "X_3"))
dml_plr_lasso <- DoubleMLPLR$new(dml_data, ml_l = ml_l_lasso, ml_m = ml_m_lasso, n_folds = 2)

# plug data in the formula
dml_plr_lasso$fit()
theta_c_hat <- dml_plr_lasso$coef

#Comparing with Lasso 
X <- data[, -which(colnames(data) %in% c("Y"))]
X <- as.matrix(X)
reg <- cv.glmnet(x = X, y = data$Y, alpha = 1)
theta_l_1 <- coef(reg, s = "lambda.min")["T", ]
theta_l_2 <- coef(reg, s = "lambda.1se")["T", ]
results_lasso <- data.frame(
  Model = c("lowest mean cross-validated error", "one standard error", "DML"),
  TE = c(theta_l_1, theta_l_2, theta_c_hat)
)

results_lasso
```

Conclusion
```{r}
res_llr <- local_linear_regression(optimal_h_n, data)
results <- data.frame(
  Model = c("Linear", "Linear Interaction", "Quadratic", "Quadratic Interaction", "Cubic", "Cubic Interaction", "Local Linear Regression", "DML partial Linear"),
  TE = c(l_theta_c, l_int_theta_c, qua_theta_c, qua_int_theta_c, cub_theta_c, cub_int_theta_c, res_llr, theta_c_hat)
)
results

```